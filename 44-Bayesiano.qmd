## Modelado Bayesiano.

En el apéndice (ver @sec-bayes-2) se comparan diversas parametrizaciones de modelado bayesiano utilizando la función `brm()` del paquete `brms`. Se analiza aquí el mejor modelo según el criterio de validación cruzada bayesiana `leave-one-out`:

\scriptsize
```{r}
#| echo: true
#| cache: true
brm_treat.period.subject.question <- brm(
    Response ~ Treat * Period + (1 + Treat | Subject) + (1 + Treat | Item),
    data = df_clean,
    family = cumulative("logit"),
    sample_prior = TRUE,
    file = "models/brm_treat.period.subject.question",
    file_refit = "on_change"
)
```
\normalsize

este modelo coincide con el seleccionado en el apartado de Regresión Ordinal con efectos mixtos. El modelo utiliza como factores con efectos fijos (`complete pooling` en terminología bayesiana) el nivel de subtitulado y el periodo y la interacción entre ambos; y como efectos aleatorios (`partial pooling`) los sujetos y las preguntas del test. Cada uno de ellos con un intercepto y un nivel de subtitulado variable. El resumen del modelo es el siguiente:

\scriptsize
```{r}
#| echo: true
summary(brm_treat.period.subject.question)
```
\normalsize

En la @tbl-model-comp se comparan las estimaciones puntuales que se obtuvo para este modelo con la función `clmm` para ajustar el modelo con efectos mixtos. Se comprueba que son muy similares. Los interceptores son claramente significativos y también el coeficiente de `TreatB`. Sin embargo los coeficientes correspondientes al efecto periodo, `Period2`, y al efecto secuencia, `TreatB:Period`, incluyen el cero y además tienen intervalos muy grandes por lo que hay mucha incertidumbre respecto a su verdadero valor.  

\tiny
```{r}
#| label: tbl-model-comp
#| tbl-cap: Comparación frecuentista/bayesiano de coeficientes estimados en el modelo Response ~ Treat * Period + (1 + Treat | Subject) + (1 + Treat | Item).
#| cache: true
df_clmm <- coef(clmm_treat.period.subject.question) %>% data.frame()
colnames(df_clmm) <- c("Estimation.clmm")

df_clmm <- bind_cols(df_clmm, confint(clmm_treat.period.subject.question))

df_clmm_ranef_coefs <- c(
    attr(VarCorr(clmm_treat.period.subject.question)$Item, "stddev")[1],
    attr(VarCorr(clmm_treat.period.subject.question)$Item, "stddev")[2],
    attr(VarCorr(clmm_treat.period.subject.question)$Subject, "stddev")[1],
    attr(VarCorr(clmm_treat.period.subject.question)$Subject, "stddev")[2],
    attr(VarCorr(clmm_treat.period.subject.question)$Item, "correlation")[2, 1],
    attr(VarCorr(clmm_treat.period.subject.question)$Subject, "correlation")[2, 1]
) %>% data.frame()

rownames(df_clmm_ranef_coefs) <- c("Subject.sd(Intercept)", "Subject.sd(TreatB)", "Subject.cor(Intercept,TreatB)", "Item.sd(Intercept)", "Item.sd(TreatB)", "Item.cor(Intercept,TreatB)")
colnames(df_clmm_ranef_coefs) <- c("Estimation.clmm")

df_clmm <- bind_rows(df_clmm, df_clmm_ranef_coefs)
df_brm <- (broom.mixed::tidyMCMC(brm_treat.period.subject.question, conf.int = T))[1:13, ]

colnames(df_clmm) <- c("Estimation.clmm", "conf.2.5%", "conf.97.5%")
colnames(df_brm) <- c("Name", "Estimation.brm", "std.error", "cred.2.5%", "cred.97.5%")
df_brm$Name <- rownames(df_clmm)
df_clmm$Name <- df_brm$Name
df_clmm <- df_clmm %>% tibble()

left_join(df_clmm, df_brm) %>%
    relocate(Name) %>%
    mutate(across(where(is.numeric), ~ round(., 2))) %>%
    select(-c("std.error")) %>%
    gt() %>%
    tab_spanner(
        label = "ordinal::clmm",
        columns = c(Estimation.clmm, `conf.2.5%`, `conf.97.5%`)
    ) %>%
    tab_spanner(
        label = "brms::brm",
        columns = c(Estimation.brm, `cred.2.5%`, `cred.97.5%`)
    ) %>%
    sub_missing(columns = everything(), missing_text = "") %>%
    tab_options(table.font.size = 6)
```
\normalsize

Se han mantenido las distribuciones de probabilidad a priori que por defecto utiliza `brm` confiando en que sus parámetros son adecuados. Sin embargo, conviene comprobar que realmente sea así. En la @tbl-priors se muestran las distribuciones a priori de los parámetros aleatorios del modelo. En la @fig-priors se constata que toman valores razonables y no informativos.

\tiny
```{r}
#| label: tbl-priors
#| tbl-cap: Distribuciones a priori del modelo Response ~ Treat * Period + (1 + Treat | Subject) + (1 + Treat | Item).
#| tbl-scap: Distribuciones a priori del modelo seleccionado.
#| cache: true
#| fig-width: 8
prior_summary(brm_treat.period.subject.question) %>%
    gt() %>%
    tab_options(table.font.size = 8)
```
\normalsize

```{r}
#| label: fig-priors
#| fig-cap: Distribuciones a priori del modelo Response ~ Treat * Period + (1 + Treat | Subject) + (1 + Treat | Item).
#| fig-scap: Distribuciones a priori del modelo seleccionado.
prior_draws(brm_treat.period.subject.question) %>%
    pivot_longer(cols = everything(), names_to = "term", values_to = "value") %>%
    ggplot(aes(x = value, y = term, fill = term)) +
    geom_violin() +
    labs(y = NULL) +
    coord_cartesian(xlim = c(-7.5, 7.5)) +
    theme(legend.position = "none")
```

Es importante asegurar que el entrenamiento ha convergido a su distribución a posteriori. En la tabla de resumen se constata que el valor de `Rhat` es inferior a 1.1 y el de `ESS` superior a 400 en todos los parámetros, que son umbrales que no se deberían violar [ver @burkner2019]. En la
 @fig-trace se comprueba que las cadenas MCMC de muestreo de la distribución a posteriori se mezclan correctamente y no se aprecia autocorrelación en ninguno de las parámetros. Por último, en la @fig-predictive se muestra una comparación entre los histogramas construidos con los datos con los intervalos de confianza marginales de la función predictiva a posteriori del modelo. En la mayoría de las preguntas, el muestreo reproduce bastante bien el histograma de respuestas. En algunas preguntas, como la `Q16` o la `Q17`, hay diferencias relevantes.
 
```{r}
#| label: fig-trace
#| fig-cap: Cadenas MCMC del modelo Response ~ Treat * Period + (1 + Treat | Subject) + (1 + Treat | Item).
#| cache: true
#| fig-height: 10
mcmc_plot(brm_treat.period.subject.question, type = "trace")
```

```{r}
#| label: fig-predictive
#| fig-cap: Comparación de los valores reales con los obtenidos a partir de la función predictiva a posteriori del modelo Response ~ Treat * Period + (1 + Treat | Subject) + (1 + Treat | Item).
#| fig-height: 10
pp_check(brm_treat.period.subject.question, type = "bars_grouped", group = "Item", ndraws = 1000)
```

En cuanto a las conclusiones que se pueden extraer del modelo, la más importante es que, como se ha constatado desde el principio de este trabajo, los alumnos perciben claramente una diferencia entre los niveles de subtitulado $A$ y $B$, siendo preferido el $A$. En la @fig-predictive-2 se representan gráficamente los valores esperados de la probabilidad de respuesta a cada pregunta por tratamiento y periodo. Lo más interesante es confirmar la existencia de sendos efectos periodo y secuencia de pequeña magnitud que se materializan en que, para el nivel de subtitulado $A$, son ligeramente más probables las respuestas con valor 4 en el periodo 2 (secuencia $BA$) que en el 1 (secuencia $AB$). Con las respuestas de valor 5 ocurre lo contrario: en el subtitulado $A$ son más probables en el periodo 1 (secuencia $AB$) que el 2 (secuencia $BA$). Esto ya se había constatado antes en el análisis del $OR$ (ver @sec-or).


```{r}
#| label: fig-predictive-2
#| fig-cap: Muestreo de la esperanza de la función predictiva a posteriori por tratamiento, periodo y pregunta del modelo Response ~ Treat * Period + (1 + Treat | Subject) + (1 + Treat | Item).
#| fig-height: 10
#| cache: true
newdata <- df_clean %>% modelr::data_grid(Period, Treat, Item)
pred_brm <- brm_treat.period.subject.question %>%
    epred_draws(newdata = newdata, re_formula = ~ (1 + Treat | Item), by = c("Treat", "Item"), category = "Response") %>%
    select(Period, Treat, Item, Response, .epred) %>%
    group_by(Period, Treat, Item, Response) %>%
    summarize(
        lo = quantile(.epred, 0.025),
        Probability = median(.epred),
        hi = quantile(.epred, 0.975)
    ) %>%
    ungroup()

pred_brm %>% ggplot(aes(x = Treat, y = Probability, colour = Response)) +
    geom_point(aes(shape = Period)) +
    geom_line(aes(group = interaction(Period, Response), linetype = Period)) +
    geom_ribbon(data = pred_brm, aes(ymin = lo, ymax = hi, fill = Response, group = Response), colour = NA, alpha = 0.2) +
    facet_grid(Item ~ Response) +
    theme(panel.spacing = grid::unit(0, "lines"), legend.position = "top") +
    scale_x_discrete(expand = c(0.05, .05), limits = c("A", "B"))
```

Contrastados los efectos periodo y secuencia, y sabiendo que son de pequeña magnitud, se hace ahora un análisis de las predicciones que realiza el modelo sobre la distribución de respuestas en cada pregunta. Para ello en la @fig-pred-3 se representan 50 muestras de la esperanza de la distribución predictiva a posteriori para cada pregunta y nivel de subtitulado marginalizadas por periodo y estudiante. La primera conclusión que se puede extraer es que el modelo tiene bastante incertidumbre sobre los valores de respuesta a cada pregunta no superando casi nunca el 50% de probabilidad para todas las preguntas y niveles de subtitulado. En general se observa en la mayoría de las preguntas del nivel de subtitulado $A$ que los alumnos están bastante seguros de que la respuesta a las preguntas debe ser 4 ó 5, asignando una muy baja probabilidad a los valores 1, 2, ó 3, pero habiendo bastante confusión respecto cuál de los dos valores (4 ó 5) asignar. En el nivel de subtitulado $B$ la situación es bastante más confusa. Aunque la opción de respuesta preferida es 4 y las menos preferidas son la 5 y la 1, hay bastante mezcla entre las opciones de respuesta 2, 3 y 4. En cuanto al análisis individualizado por pregunta extraen las siguientes conclusiones:

* En las preguntas $Q04$ y $Q13$ los estudiantes no aprecian defectos en el subtitulado ni diferencias entre un nivel y otro. Son valoradas en ambos niveles con puntuaciones de 4 y de 5.

* En las preguntas $Q15$, $Q16$ y $Q17$, la opción de respuesta más probable es 4 en ambos subtitulados. El modelo asigna una baja probabilidad de respuesta a la opción 1 y similares al resto. La probabilidad de la opción 5 decrece ligeramente entre subtitulado $A$ y $B$ y lo contrario ocurre con las opciones 2 y 3.

* Las preguntas $Q01$, $Q02$, $Q03$, $Q10$, $Q11$ y $Q12$ son similares a las anteriores. Particularmente en lo referente a que la respuesta más probable en el subtitulado $B$ es 4. En el subtitulado $A$ hay preferencia por 4 y 5. El nivel 5 cae acusadamente en el subtitulado $B$ y en este nivel aumenta ligeramente la probabilidad de respuesta 2 y 3.

* Las preguntas $Q06$, $Q07$, $Q14$ y $Q18$ no son muy diferentes de las anteriores. En general el modelo predice mayor probabilidad de respuesta para 5 en el subtitulado $A$ pero este valor es con alta probabilidad cercano a cero en el subtitulado $B$. En el subtitulado $B$ la probabilidad de respuesta 2, 3 ó 4 es similar.

* Las preguntas $Q05$, $Q08$ y $Q09$ son las que más diferencias entre subtitulados presentan. La respuesta más probable en el subtitulado $A$ es 5 (en $Q08$ y en $Q09$ muy parecida a 4). Por contra, en el subtitulado $B$ las respuestas 4 y 5 tienden a cero, siendo la más probable la respuesta 2. En $Q05$ y en $Q09$ la segunda respuesta más probable al subtitulado $B$ es 1 y 4 en la $Q08$.

En definitiva, nuestro modelo predice que los estudiantes están bastante de acuerdo en que en las preguntas $Q05$ y $Q09$ hay una diferencia de calidad importante entre subtitulados. También están de acuerdo en que en las preguntas $Q04$ y $Q13$ no hay apenas cambio entre los subtitulados. En las preguntas $Q15$, $Q16$ y $Q17$ hay una gran confusión en ambos niveles de subtitulado y en el resto la confusión se circunscribe al nivel de subtitulado $B$, ya que en el nivel $A$ las opciones 4 y 5 predominan.

![Muestreo de la función predictiva a posteriori por tratamiento y pregunta del modelo Response ~ Treat * Period + (1 + Treat | Subject) + (1 + Treat | Item).](images/bayes-preg.png){#fig-pred-3 width=100%}


```{r}
#| fig-height: 10
#| fig-width: 8
#| cache: true
#| eval: false
pred2_brm <- brm_treat.period.subject.question %>%
    epred_draws(ndraws = 50, newdata = newdata, re_formula = ~ (1 + Treat | Item), by = c("Treat", "Item"), category = "Response") %>%
    select(Period, Treat, Item, Response, Probability = .epred, .draw) %>%
    mutate(Median = ave(Probability, FUN = function(x) quantile(x, .5, type = 3, na.rm = TRUE))) %>%
    ungroup() %>%
    group_by(.draw, Response) %>%
    mutate(indices = cur_group_id()) %>%
    ungroup()

colors <- viridis(
    option = "plasma",
    begin = 0,
    end = 0.9,
    direction = -1,
    n = 5
)

questions_vector <- setNames(levels(df_clean$Item_lr), levels(df_clean$Item))

question_labeller <- function(string) paste0(string, ": ", questions_vector[string])

# Plotting the fitted draws
p <- pred2_brm %>%
    ggplot(aes(
        x = Treat,
        y = Probability,
        color = Response,
        # Don't forget the indices!
        group = indices
    )) +
    facet_wrap(~Item, nrow = 6, labeller = as_labeller(question_labeller)) +
    geom_line(alpha = 0.4) +
    scale_color_manual(values = colors) +
    # We won't need these
    guides(
        color = FALSE,
        label = FALSE,
        scale = "none"
    ) +
    theme_ipsum_ps(base_family = NULL)
p +
    labs(
        x = "Treat",
        y = "Probability"
    ) +
    theme(
        plot.margin = margin(0, 100, 0, 0),
    ) +
    # This allows any labels or data to go past the grid
    coord_cartesian(clip = "off") +
    # Finally, our labels. We filter the data to avoid having a million of them
    geom_text_repel(
        data = pred2_brm %>% filter(Probability == Median & Item %in% c("Q02", "Q05", "Q08", "Q11", "Q14", "Q17") & Period == 2, Treat == "B") %>% distinct(Treat,
            Period, Item, Response,
            .keep_all = TRUE
        ) %>% mutate(Response_l = ordered(Response, labels = levels(df_clean$Response_l))),
        aes(label = Response_l),
        direction = "y",
        hjust = 0,
        segment.size = 0.2,
        # Move the labels to the right
        nudge_x = 0.1,
        na.rm = TRUE,
        # Expand limits so that the label doesn't get stuck
        xlim = c(0, 5),
        # Adjust size as needed!
        size = 3.5
    ) + scale_x_discrete(expand = c(0, 0), limits = c("A", "B"))

```