---
title: "Ordinal regression in R: part 1"
description: |
  A theoretical and applied walkthrough of ordinal regression.
  Part 1: the frequentist approach with `ordinal`.
date: 2020-03-15
params:
  date: 2020-03-15
  slug: "ordinal-regression-in-r-part-1"
categories:
  - R
  - regression
  - ordinal
  - frequentist statistics
image: preview.png
bibliography: references.bib
lang: es

execute:
  echo: false
format:
    pdf:
        fig-pos: 'h'
---

```{r}
#| warning: false
#| include: false
#| echo: false
#| cache: false

library(car)
library(dplyr)
library(forcats)
library(ggplot2)
library(ggthemes)
library(lme4)
library(lmerTest)
library(magrittr)
library(multcomp)
library(ordinal)
library(purrr)
library(stringr)
library(readr)
library(testit)
library(tidyr)
library(tidyverse)
```

```{r}
#| warning: false
#| include: false
#| echo: false
#| cache: false

#| echo: false

# Leemos el tibble preprocesado
test_df <- read_delim("./data/preprocess/test.csv", delim = ",", show_col_types = FALSE)


# Eliminamos aquellos usuarios que no han hecho uno de los test
test_df %<>%
    group_by(User) %>%
    mutate(Rows = n()) %>%
    filter(Rows > 1) %>%
    ungroup()

# Renombramos columnas de acuerdo con Lawson y ponemos en formato largo
test_df



##### SAVE TO FILE #####

write_csv(test_df, "./data/preprocess/test.csv")

```

```{r}
#| echo: false
df <- test_df %>%
    mutate(Period = as.factor(if_else(Test == "01", 1, 2)), Treat = as.factor(if_else(Group == "A" & Test == "01" | Group == "B" & Test == "02", "A", "B")), Group = as.factor(if_else(Group == "A", "AB", "BA")), Subject = as.factor(User)) %>%
    dplyr::select(Group, Period, Treat, Subject, starts_with("Q")) %>%
    mutate_at(vars(starts_with("Q")), ~ (. + 1) %% 6) %>%
    pivot_longer(cols = all_of(starts_with("Q")), names_to = "Question", values_to = "Response") %>%
    mutate(Question = as.factor(Question), Response = as.factor(Response)) %>%
    arrange(Subject, Period, Question)

write_csv(df, "./data/preprocess/test_lg.csv")

df_clean <- df %>% filter(Response != 0)
df_0 <- df %>% filter(Response == 0)
```



```{r}
#| code-fold: true
#| code-summary: "R setup"
#| message: false

library(tidyverse)
library(dunnr)
library(gt)
library(broom)
library(patchwork)

extrafont::loadfonts(device = "all", quiet = TRUE)
# theme_set(theme_td(base_family = "Kalimati"))
set_geom_fonts()
set_palette()

wine_red <- "#58181F"
update_geom_defaults("point", list(color = wine_red))
update_geom_defaults("line", list(color = wine_red))
```



# Regresión ordinal

## Introducción

El test de Likert es una escala ordinal. Tratar las respuestas a un test de Likert como si fueran cuantitativas como se hizo en el análisis de la varianza del apartado anterior no es correcto por las siguientes razones:

* Los niveles de respuesta pueden no ser equidistantes: la distancia entre un par de opciones de respuesta puede no ser la misma para todos los pares de opciones de respuesta. Por ejemplo, la diferencia entre "Muy en desacuerdo" y "En desacuerdo" puede ser mucho menor para un encuestado que la diferencia entre "De acuerdo" y "Muy de acuerdo".

* La distribución de las respuestas ordinales puede ser no normal. En particular esto sucederá si hay frecuencias altas de respuesta en los extremos del cuestionario.

* Las varianzas de las variables no observadas que subyacen a las variables ordinales observadas pueden diferir entre grupos, tratamientos, periodos, etc. 

En @kruschke2018328 se han analizado los problemas que puede ocasionar tratar datos ordinales como si fueran cuantitativos constatando que se pueden presentar las siguientes situaciones:

* Se pueden encontrar diferencias significativas entre grupos cuando no las hay: Error tipo I.
* Se pueden obviar diferencias cuando en realidad sí existen: Error tipo II.
* Incluso se pueden invertir los efectos de un tratamiento.
* También puede malinterpretarse la interacción entre factores.

Todos estos problemas pueden ser tratados con la regresión ordinal.

## Variantes de la regresión ordinal.

Según @burkner hay tres clases de regresión ordinal:

* Regresión ordinal acumulativa.
* Regresión ordinal secuencial.
* Regresión ordinal adyacente.

Nos centraremos en la primera ya que es el más habitual y adecuado para nuestro caso.


```{r}
#| freeze: true
x <- seq(-4, 4, length.out = 500)
y <- dnorm(x, 0, 1.5)

x_cuts <- c(-2.5, -1.1, 1.8)
x_labels <- c(expression(tau[1]), expression(tau[2]), expression(tau[3]))

data <- tibble(x, y)

g <- data %>% ggplot(aes(x = x, y = y)) +
    geom_line() +
    theme(
        axis.line = element_line(color = "black", linewidth = 1),
        panel.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.line.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.text.x = element_text(color = "black", size = 12, hjust = 0.5, vjust = -3)
    ) +
    ylab(NULL)

cumulative <- g + geom_vline(xintercept = x_cuts) +
    geom_label(size = 2, data = data.frame(x = x_cuts - 0.4, y = rep(0.22, 3), label = paste("Y = ", 1:3, sep = "")), aes(x = x, y = y, label = label)) +
    geom_label(size = 2, x = 2.5, y = 0.22, label = "Y = 4") +
    scale_x_continuous(breaks = x_cuts, labels = x_labels) +
    xlab(expression(tilde(Y))) +
    coord_fixed(ratio = 5)


sequencial <- list()
for (i in 1:3) {
    p <- g + geom_vline(xintercept = x_cuts[i]) +
        geom_label(size = 2, x = x_cuts[i] - 1, y = 0.22, label = paste("Y = ", i, sep = "")) +
        geom_label(size = 2, x = x_cuts[i] + 1, y = 0.22, label = paste("Y > ", i, sep = "")) +
        scale_x_continuous(breaks = x_cuts[i], labels = x_labels[i]) +
        xlab(bquote(tilde(Y)[.(i)])) +
        coord_fixed(ratio = 30)
    sequencial[[i]] <- p
}

adjacent <- list()
for (i in 1:3) {
    p <- g + geom_vline(xintercept = x_cuts[i]) +
        geom_label(size = 2, x = x_cuts[i] - 1, y = 0.22, label = paste("Y = ", i, sep = "")) +
        geom_label(size = 2, x = x_cuts[i] + 1, y = 0.22, label = paste("Y = ", i + 1, sep = "")) +
        scale_x_continuous(breaks = x_cuts[i], labels = x_labels[i]) +
        xlab(bquote(tilde(Y)[.(i)])) +
        coord_fixed(ratio = 30)
    adjacent[[i]] <- p
}

```


El modelo acumulativo, CM, presupone que la variable ordinal observada, $Y$, proviene de la categorización de una variable latente (no observada) continua $\tilde{Y}$. Hay $K$ umbrales $\tau_k$ que particionan $\tilde{Y}$ en $K + 1$ categorías ordenadas observables. Si asumimos que $\tilde{Y}$ tiene una cierta distribución (por ejemplo, normal) con distribución acumulada $F$, se puede calcular la probabilidad de que $Y$ sea la categoría $k$ de esta forma:

$$Pr(Y = k) = F(\tau_k) - F(\tau_{k-1})$$

Por ejemplo en la @fig-cumulative,

$$Pr(Y = 2) = F(\tau_2) - F(\tau_{1})$$

Si suponemos que, por ejemplo:

$$\tilde{Y} = \eta + \epsilon = b_1 x_1 + b_2 x_2 + \epsilon$$

Y que los errores son $N(0,\sigma^2)$.

Entonces:

$$\mathrm{Pr}(\epsilon \leq z) = F(z)$$

Y:

$$\mathrm{Pr}(Y \leq k \mid \eta) = \mathrm{Pr}(\tilde{Y} \leq \tau_k \mid \eta) = \mathrm{Pr}(\eta + \epsilon \leq \tau k) = \mathrm{Pr}(\epsilon \leq \tau_k - \eta) = F(\tau_k - \eta)$$

Por lo que:

$$\mathrm{Pr}(Y = k) = \Phi(\tau_k - (b_1 x_1 + b_2 x_2)) - \Phi(\tau_{k - 1} - (b_1 x_1 + b_2 x_2))$$

Donde hay que estimar los umbrales y los coeficientes de regresión.

Otra popular elección es suponer que la función acumulada se comporta como una logística. En ese caso, la interpretación de los coeficientes varía y se asemeja a la de la regresión logística. Se parte del supuesto de que:

$$logit (P(Y \le k)) = \tau_{k} - \eta = \tau_{k} - (b_1 x_1 + b_2 x_2)$$

Se puede demostrar que, por ejemplo:

$$\frac{\frac{\mathrm{Pr}(Y \leq k_1 \mid \eta)}{\mathrm{Pr}(Y > k_1 \mid \eta)}}{\frac{\mathrm{Pr}(Y \leq k_2 \mid \eta)}{\mathrm{Pr}(Y > k_2 \mid \eta)}} = \exp(\tau_{k_1} - \tau_{k_2})$$



```{r}
#| label: fig-cumulative
#| fig-cap: Regresión ordinal acumulativa.
#| freeze: true
cumulative
```

```{r}
#| include: false
renv::use(lockfile = "renv.lock")
```


## Preparación


```{r}
#| message: false
library(ordinal)
df <- as_tibble(df_clean)
glimpse(df)
```

```{r}
df_resume <- df %>%
    dplyr::select(Subject, Period, Group, Treat, Response, Question) %>%
    group_by(Group, Period, Treat) %>%
    count(Response) %>%
    ungroup()

min_freq_response <- min(df_resume$n)
max_freq_response <- max(df_resume$n)
```

```{r}
#| label: resume
#| tbl-cap: Resumen de frecuencias de respuesta.
#| fig-pos: h
df_resume %>%
    pivot_wider(id_cols = c(Group, Period, Treat), names_from = Response, values_from = n) %>%
    gt() %>%
    tab_spanner(columns = `1`:`5`, label = "Response") %>%
    data_color(
        columns = `1`:`5`,
        colors = scales::col_numeric(
            palette = c("white", wine_red), domain = c(min_freq_response, max_freq_response)
        )
    )
```

```{r}
#| label: resume2
#| fig-cap: Resumen de frecuencias de respuesta.
#| fig-pos: h
df_resume %>%
    ggplot(aes(x = Treat, y = Response, color = Treat)) +
    geom_point(aes(group = Treat, size = n)) +
    facet_wrap(~Group,
        scales = "free_x"
    ) +
    add_facet_borders() +
    theme_bw() +
    theme(
        legend.position = "none", panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()
    )
```



## Modelo de enlace logit acumulado


Vamos a ajustar el modelo con la función de enlace logit:

$$
\text{logit} (P(y_i \leq k) = \log \frac{P(y_i \leq k)}{1 - P(y_i \leq k)}
$$ {#eq-link-logit}

La función de enlace logit acumulada (@eq-link-logit) no está definida para $k = K$, ya que $1 - P(Y_i \leq K) = 1 - 1 = 0$.

En nuestra escala de Likert tenemos $K$ = 5 niveles, el modelo mixto que vamos a plantear es el siguiente:

$$
\begin{aligned}
\text{logit}(p(y_i \leq )k) &= \tau_k - \beta_1 \text{Period}_i - \beta_2 \text{Treat}_i - u( \text{Subject}_i) - v( \text{Question}_i) \\
i &= 1, \dots n \; \; \; \; \; \; k = 1, \dots, K - 1
\end{aligned}
$$

donde $\tau_k$ es el umbral de la categoría $k$ y son $K-1$ = 4 interceptores.
Los coeficientes de los efectos fijos, $\beta_1$ and $\beta_2$, son independientes $k$, por lo que cada $\beta$ tiene el mismo efecto en los $K-1$ logits acumulados.
Los efectos aleatorios, Subject y Question, también son independientes de $k$, y se presupone que siguen una distribución normal: $u(\text{Subject}_i) \sim N(0, \sigma_u^2)$ y $u(\text{Question}_i) \sim N(0, \sigma_v^2)$ respectivamente.

En esencia lo que estamos haciendo es un modelo en cadena de regresiones logísticas donde la respuesta binaría se corresponde con "menor o igual que cierto nivel frente a mayor que ese nivel".

En el caso particular de $K$ = 5, los umbrales $\tau_k$ se interpretan como:

* $k$ = 1: log-odds del nivel = 1 vs. 2-5
* $k$ = 2: log-odds del nivel = 1-2 vs. 3-5
* $k$ = 3: log-odds del nivel = 1-3 vs. 4-5
* $k$ = 4: log-odds del nivel = 1-4 vs. 5


