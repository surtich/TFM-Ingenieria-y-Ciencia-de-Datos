{
  "hash": "38d51ed47b7e48b6ca58e644be7021b2",
  "result": {
    "markdown": "::: {.cell}\n\n:::\n\n\n# Marco teórico y estado del arte {#sec-arte}\n\n## Características del diseño del experimento\n\nEn este trabajo se estudiarán las diferencias existentes entre dos niveles de subtitulado (uno correcto y otro con errores) a través de las respuestas de los estudiantes a una escala de Likert de 18 ítems que fue respondida tras visualizar cada vídeo (ver @sec-desc). Para ello se propondrán modelos estadísticos adecuados al diseño del experimento.\n\nEl diseño del experimento de la actividad de subtitulado fue completamente aleatorizado y **cruzado *AB/BA***. En estos diseños se desea conocer el efecto de un factor con dos niveles sobre una variable respuesta. Para ello, se asigna aleatoriamente a los participantes a dos grupos y se mide la variable respuesta en dos periodos en cada grupo. En el primer periodo, a uno de los grupos se le asigna un nivel de factor y al otro grupo el otro nivel de factor. En el segundo periodo se intercambian los niveles de factor asignados a cada grupo (ver @fig-ab-ba). Este diseño se diferencia del diseño paralelo $AA/BB$ en el que el nivel de factor asignado a cada grupo se mantiene entre periodos (ver @fig-aa-bb). El \\gls{diseño cruzado} y paralelo se pueden combinar si los participantes se clasifican en cuatro grupos ($AA/AB/BA/BB$). Por último, se pueden hacer diseños con más periodos o/y con más factores o niveles de factor. Los diseños cruzados son habituales en estudios clínicos en investigación médica [ver @lim2021] y farmacológica para la evaluación de medicamentos genéricos.\n\n\n\n```{mermaid}\n%%| label: fig-ab-ba\n%%| fig-cap: Diagrama de diseño cruzado AB/BA\n%%| fig-width: 6\nflowchart LR\n  subgraph Primer Periodo\n  C[Tratamiento A]\n  D[Tratamiento B]\n  end\n  subgraph Segundo Periodo\n  E[Tratamiento B]\n  F[Tratamiento A]\n  end\n  A[Sujetos participantes] --> B{Aleatorización}\n  B-- Secuencia AB --> C-.->E\n  B-- Secuencia BA --> D-.->F\n```\n\n```{mermaid}\n%%| label: fig-aa-bb\n%%| fig-cap: Diagrama de diseño paralelo AA/BB\n%%| fig-width: 6\nflowchart LR\n  subgraph Primer Periodo\n  C[Tratamiento A]\n  D[Tratamiento B]\n  end\n  subgraph Segundo Periodo\n  E[Tratamiento A]\n  F[Tratamiento B]\n  end\n  A[Sujetos participantes] --> B{Aleatorización}\n  B-- Secuencia AA --> C-.->E\n  B-- Secuencia BB --> D-.->F\n```\n\n\n\nUn **\\gls{diseño completamente aleatorizado}** [@lawson2015, pp. 18] \"garantiza la validez del experimento contra sesgos causados por otras variables ocultas. Cuando las unidades experimentales se asignan aleatoriamente a los niveles de factor de tratamiento, se puede realizar una prueba exacta de la hipótesis de que el efecto del tratamiento es cero utilizando una prueba de aleatorización\".\n\nSiguiendo a @senn2022 [pp. 5-9], para que el ensayo sea de tipo cruzado no sería suficiente intercambiar las secuencias sino que debe ser objeto del ensayo el estudio de las diferencias entre los tratamientos individuales que componen las secuencias. En la misma línea, @lui2016 [pp. 1-2] afirma que \"el objetivo principal de un diseño cruzado es estudiar las diferencias entre tratamientos individuales (en lugar de las diferencias entre secuencias de tratamiento). Debido a que cada paciente sirve como su propio control, el diseño cruzado es una alternativa útil al diseño de grupos paralelos para aumentar la potencia\".\n\nLos principales problemas de un diseño cruzado son el abandono (`drop-out`), de alguno de los participantes y la interacción entre el tratamiento y el periodo o efecto secuencia (`carry-over` o contaminación). Además, el análisis estadístico es más complicado, particularmente cuando la respuesta es ordinal y hay más de dos tratamientos. Aplicado al experimento del subtitulado, se producirá **efecto secuencia** si las respuestas a los cuestionarios fueran diferentes cuando los vídeos se ven en un orden que cuando se ven en el otro. Además hay que tener en consideración de la existencia del **efecto periodo**, que se producirá si las respuestas del segundo periodo están influidas por haber realizado la primera actividad de subtitulado. Como ejemplo de estos efectos se puede ver @senn2022 [pp. 35-53]: Se analiza un experimento que consistió en medir el flujo espiratorio máximo (PEF, por sus siglas en inglés) en 13 niños con edades entre los 7 y los 14 años con asma a los que se les administró salbutamol (un conocido broncodilatador) y formoterol (un broncodilatador de reciente aparición en el momento en que se realizó el estudio). Se hicieron dos grupos a los que se les administró ambos tratamientos en orden inverso dejando un periodo de lavado (`washout period`) entre aplicaciones. El \\gls{efecto periodo} resulta de medir si el PEF medio de los dos grupos es diferente entre periodos, y el \\gls{efecto secuencia} consiste en comprobar si hay diferencias significativas entre aplicar primero el tratamiento con salbutamol y luego con formoterol o hacerlo al revés.\n\nOtra cuestión de relevancia es que las respuestas a los ítems de una \\gls{escala de likert} son de **tipo ordinal**. Los test estadísticos *\\gls{ANOVA}* o *\\gls{MANOVA}* presuponen que la variable de respuesta es cuantitativa y con distribución normal. Tratar las respuestas a una escala de Likert como si fueran cuantitativas no es correcto por las siguientes razones:\n\n* Los niveles de respuesta no son necesariamente equidistantes: la distancia entre cada par de opciones de respuesta correlativos puede no ser la misma para todos los pares. Por ejemplo, la diferencia entre \"Muy en desacuerdo\" y \"En desacuerdo\" y la diferencia entre \"De acuerdo\" y \"Muy de acuerdo\" es de un nivel, pero psicológicamente puede ser percibida de forma diferente por cada sujeto.\n\n* La distribución de las respuestas ordinales puede ser no normal. En particular esto sucederá si hay muchas respuestas en los extremos del cuestionario.\n\n* Las varianzas de las variables no observadas que subyacen a las variables ordinales observadas pueden diferir entre grupos, tratamientos, periodos, etc. \n\nEn @kruschke2018 se han analizado los problemas potenciales de tratar datos ordinales como si fueran cuantitativos constatando que se pueden presentar las siguientes situaciones:\n\n* Se pueden encontrar diferencias significativas entre grupos cuando no las hay: \\gls{error de tipo I}.\n* Se pueden obviar diferencias cuando en realidad sí existen: \\gls{error de tipo II}.\n* Incluso se pueden invertir los efectos de un tratamiento.\n* También puede malinterpretarse la interacción entre factores.\n\nOtra cuestión que hay que tener en cuenta es que, al tratarse de un diseño cruzado, es de **medidas repetidas** ya que cada sujeto realiza dos veces el test, uno con cada vídeo y que, por lo tanto, las respuestas a cada test de un mismo sujeto no son independientes. Además, tampoco se pueden considerar independientes los ítems que componen el test ya que los ítems pretenden medir la misma variable latente: la calidad del subtitulado.\n\nEn este trabajo se analiza si el nivel de subtitulado (correcto o defectuoso) influye en el nivel de respuesta a los ítems de la escala de Likert, que es la variable dependiente. Se evalúa también la existencia de efectos secuencia y periodo y la influencia que tienen sobre el nivel de respuesta el estudiante y el propio ítem.\n\n## Modelos Lineales Generalizados [^GLM] {#sec-glm}\n\n[^GLM]: No se deben confundir los Modelos Lineales Generales con los Modelos Lineales Generalizados. En los primeros, también llamados Modelos de Regresión Multivariante, se presupone que las variables respuesta tienen una relación lineal con los predictores y sus valores se distribuyen normalmente. Los segundos son una generalización de los primeros y permiten que la variable respuesta admita otras distribuciones además de la normal.\n\nEl Modelo Lineal Generalizado (*Generalized Linear Model*, $GLM$) es un modelo en el que la variable respuesta no sigue una distribución Normal. Para especificar un \\gls{GLM} son necesarios tres componentes [ver @agresti_2018, pp. 66-67]:\n\n* Un componente aleatorio que será una distribución de probabilidad de la familia exponencial. Se asume que la variable respuesta $Y$ se distribuye según este componente aleatorio.\n* Un componente lineal de predictores:\n$$\n\\tau+\\beta_1x_1+...+\\beta_px_p\n$$\n\n* Una función de enlace $g$ que relaciona $\\mu=E(Y)$ con los predictores, de tal forma que:\n$$\ng(\\mu)=\\tau+\\beta_1x_1+...+\\beta_px_p\n$$\n\n\nLa estimación de coeficientes en *GLM* se realiza maximizando la función de verosimilitud (*Maximum Likelihood Estimation*, *\\gls{MLE}*). Es decir, que los coeficientes del modelo son aquellos que maximizan la probabilidad de los datos.\n\n### Regresión Logística {#sec-logistica}\n\nLa \\gls{Regresión Logística} es un caso particular de $GLM$ en el que la variable respuesta es dicotómica. Aunque la Regresión Logística no es aplicable directamente a las respuestas de una escala de Likert por ser éstas ordinales, se introduce aquí por dos motivos:\n\n* En la sección de modelado (ver @sec-logistica-2), se propondrán dos transformaciones de la variable respuesta para convertirla en dicotómica.\n* Además, en este capítulo se introducirá la Regresión Ordinal (ver @sec-ordinal). Este modelo se puede considerar una extensión de la Regresión Logística y permitirá tratar la variable respuesta como ordinal. De ahí el interés en presentar previamente la Regresión Logística.\n  \nComo se ha dicho, la \\gls{Regresión Logística} [ver @agresti_2018, pp. 68-69] es un caso particular de $GLM$ donde la variable respuesta, $Y$, es dicotómica o Bernoulli. Es decir, que $Y$ toma valores 0 ó 1. En una función de Bernoulli de parámetro $\\pi$ ($E[Y] = P(Y=1) = \\pi$), es necesaria una función que mapee los valores que puede tomar el componente lineal de rango $(-\\infty, +\\infty)$ a los valores que puede tomar $\\pi$ en el rango $(0, 1)$. Una función que permite hacer esto es la *\\gls{función logit}*:\n\n$$\nlogit(Y=1) = log \\left[\\frac{P(Y=1)}{1-P(Y=1)} \\right] = \\tau+\\beta_1x_1+...+\\beta_px_p\n$$ {#eq-logistic}\n\nLa inversa de la función *logit* es la *\\gls{función logística}* y permite realizar el mapeo inverso para obtener la probabilidad:\n\n$$\nP(Y=1) = \\frac{1}{1 + exp^{-\\tau-\\beta_1x_1...-\\beta_px_p}}\n$$\n\nLa interpretación de los coeficientes es la siguiente [ver @frienly2015, p. 260]:\n\n* $\\tau$ es el logaritmo del *\\gls{odds}* de $Y$ cuando $x_j=0, \\forall j \\in 1...p$.\n* $\\beta_j$ es el logaritmo del *\\gls{odds ratio}* asociado a una unidad de incremento de $x_j$.\n\nEl contraste de hipótesis para los coeficientes $\\beta$:\n\n$$\n\\begin{aligned}\nH_0: \\beta_j =  0 \\\\\nH_1: \\beta_j \\ne  0\n\\end{aligned}\n$$\n\nse puede realizar con el Test de Wald:\n\n$$\n\\begin{aligned}\nW & = \\frac{\\hat\\beta_j - 0}{se(\\hat\\beta_j)} \\sim N(0,1)\n\\end{aligned}\n$$\n\n\no con el Test de Razón de Verosimilitudes (*Likelihood Ratio Test*, *\\gls{LRT}*):\n\n$$\n\\begin{aligned}\nLRT = \\Lambda &= -2 \\log \\frac{L(\\widehat{reducido})}{L(\\widehat{completo})}\\\\\n&= -2 \\log L(\\widehat{reducido}) + 2 \\log L(\\widehat{completo}) \\sim \\chi^2_r\n\\end{aligned}\n$$\n\ndonde:\n\n* r es el número de $\\beta's$ iguales a cero.\n* $L(\\widehat{reducido})$ es el valor que maximiza la función de verosimilitud en la que algunos ($r$) de los $\\beta's$ han sido igualados a cero.\n* $L(\\widehat{completo})$ es el valor que maximiza la función de verosimilitud en el modelo que incluye todos los $\\beta's$.\n\n$LTR$ permite comprobar la hipótesis de que uno o varios coeficientes sean cero. Para comparar modelos no anidados, se puede usar el Criterio de Información de Akaike (AIC) o el Criterio de Información Bayesiano (BIC), que se definen respectivamente:\n\n$$\n\\begin{aligned}\nAIC &: -2 \\log L + 2p \\\\\nBIC &: -2 \\log L + p \\log(n)\n\\end{aligned}\n$$ {#eq-aic}\n\ndonde $L$ es el valor de maxima verosimilitud y el segundo sumando es una penalización que será mayor cuanto más complejo sea el modelo (*p = número de parámetros*, *n = observaciones*).   \n\n\n### Regresión Ordinal {#sec-ordinal}\n\n\n\n::: {.cell freeze='true'}\n\n:::\n\n  \nLas respuestas a los ítems de una escala de Likert son ordinales. La \\gls{Regresión Ordinal} es una clase de $GLM$ que comparte muchas similitudes con la Regresión Logística (ver @sec-logistica) pero que tiene en consideración que los valores de la variable de respuesta están ordenados\n^[Otras variantes de la Regresión Logística son la Regresión Categórica y la Regresión Multinomial. En estos tipos de *GLM* la variable respuesta puede adoptar varios valores pero no se asume que estén ordenados. La Regresión Categórica y la Regresión Multinomial están relacionadas en el mismo sentido en que lo están la Regresión Logística con función de enlace Bernoulli y con función de enlace Binomial. Es decir, que la Regresión Categórica se usa cuando las observaciones no están agrupadas y la Multinomial cuando sí lo están.]. Según @burkner2019 [pp. 3-11] hay tres clases de Regresión Ordinal:\n\n* Regresión Ordinal Acumulativa.\n* Regresión Ordinal Secuencial.\n* Regresión Ordinal Adyacente.\n\nLas regresiones ordinales secuencial y adyacente presuponen que para alcanzar un nivel se ha tenido que pasar previamente por los anteriores. En un ítem de Likert esto carece de sentido y, por lo tanto, se descartan estos modelos y se prefiere el *Modelo Acumulativo* (*Cumulative Model*, $CM$) que además es el más utilizado [ver @burkner2019, pp. 23-24].\n\n$CM$ presupone que la variable ordinal observada, $Y$, proviene de la categorización de una variable latente (no observada) continua, $\\tilde{Y}$. Hay $K$ umbrales $\\tau_k$ que particionan $\\tilde{Y}$ en $K + 1$ categorías ordenadas observables (ver @fig-cumulative). Si se asume que $\\tilde{Y}$ tiene una cierta distribución (por ejemplo, normal) con distribución acumulada $F$, se calcula la probabilidad de que $Y$ sea la categoría $k$ de esta forma:\n\n$$Pr(Y = k) = F(\\tau_k) - F(\\tau_{k-1})$$\n\n\n\n\n::: {.cell freeze='true'}\n::: {.cell-output-display}\n![Función latente en una regresión ordinal acumulativa.](2_files/figure-html/fig-cumulative-1.png){#fig-cumulative width=672}\n:::\n:::\n\n\n\nPor ejemplo en la @fig-cumulative: $Pr(Y = 2) = F(\\tau_2) - F(\\tau_{1})$. Suponiendo que $\\tilde{Y}$ tenga una relación lineal los predictores:\n\n$$\\tilde{Y} = \\eta + \\epsilon = \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\epsilon$$\n\nentonces la función de probabilidad acumulada de los errores tendrá la misma forma que la de $\\tilde{Y}$:\n\n$$P(\\epsilon \\leq z) = F(z)$$\n\nSe puede calcular la distribución de probabilidad acumulada de $Y$:\n\n$$P(Y \\leq k \\mid \\eta) = P(\\tilde{Y} \\leq \\tau_k \\mid \\eta) = P(\\eta + \\epsilon \\leq \\tau_k) = P(\\epsilon \\leq \\tau_k - \\eta) = F(\\tau_k - \\eta)$$\n\nPor lo que asumiendo la normalidad de los errores:\n\n$$P(Y = k) = \\Phi(\\tau_k - \\eta) - \\Phi(\\tau_{k - 1} - \\eta)$$\n\ndonde hay que estimar los umbrales $\\tau_k$ y las pendientes de cada variable explicativa. La función anterior es la conocida como la función de enlace `probit`. La interpretación de los coeficientes con esta función de enlace no resulta intuitiva. Por ello en este trabajo se va a utilizar la función de enlace `logit`. Con esta función de enlace la interpretación de los coeficientes es parecida a la de los coeficientes de la regresión logística. Además, en la práctica, los coeficientes estimados suelen tener valores similares a los de la función `probit`. Para entender como se deben interpretar los coeficientes del modelo $CM$ se parte del supuesto de que el $logit$ de la función de probabilidad es lineal:\n\n$$\nlogit [P(Y \\le k)] = \\tau_{k} - \\eta = \\tau_{k} - (\\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p)\n$$ {#eq-ordinal}\n\nEn ese caso, se puede demostrar fácilmente que, por ejemplo:\n\n$$\\frac{\\frac{P(Y \\leq k \\mid \\eta)}{P(Y > k \\mid \\eta)}}{\\frac{P(Y \\leq k+1 \\mid \\eta)}{P(Y > k+1 \\mid \\eta)}} = \\exp(\\tau_{k} - \\tau_{k+1})$$\n\nY que\n^[En la @sec-ordinal-2 se demuestra esta fórmula.]:\n\n$$\\frac{\\frac{P(Y \\leq k \\mid x_j = 1)}{P(Y > k \\mid x_j = 1)}}{\\frac{P(Y \\leq k \\mid x_j=0)}{P(Y > k \\mid x_j = 0)}} = \\exp(-\\beta_{j})$$\n\no, equivalentemente:\n\n$$\\frac{\\frac{P(Y > k \\mid x_j = x + 1)}{P(Y \\leq k \\mid x_j = x + 1)}}{\\frac{P(Y > k \\mid x_j = x)}{P(Y \\leq k \\mid x_j = x)}} = \\exp(\\beta_{j})$$\n\nEs decir, que $\\exp(\\beta_{j})$ es el *odds ratio* (cambio relativo entre $odds$, \\gls{OR}) de que la variable respuesta esté por encima de una determinada categoría versus estar por debajo de ella para una unidad de incremento del predictor $x_j$. Un valor del coeficiente $\\beta_j$ positivo indica que la relación entre el predictor $x_j$ y la función de $logit$ es positiva y, por lo tanto, se incrementa la probabilidad de un mayor valor de la variable respuesta.\n\n#### Presunciones del modelo\n\nEste modelo se denomina proporcional ya que se asume que cada predictor tiene los mismos efectos sobre todos los niveles de la variable de respuesta ordinal [ver @Liu2202, chap. 5]. Es decir, que los $odds$ de los niveles de respuesta deben ser proporcionales para los mismos valores de las variables explicativas. Esta suposición frecuentemente no es realista y se puede relajar permitiendo estimar un coeficiente diferente para cada nivel de la variable respuesta. Sin embargo, el incremento del número de coeficientes dificulta la interpretabilidad del modelo. @harrell2020 aboga por usar este modelo incluso aunque la suposición de proporcionalidad no se cumpla:\n\n> \"Ningún modelo se ajusta perfectamente a los datos, ..., la aproximación ofrecida por el modelo $CM$ sigue siendo bastante útil. Y un análisis unificado del modelo $CM$ es decididamente mejor que recurrir a análisis ineficientes y arbitrarios de valores dicotomizados de Y.\"\n\nMatemáticamente la presunción de la proporcionalidad de los $odds$ se demuestra a partir de la @eq-ordinal. Si se fijan los predictores en un valor arbitrario $X=x$ y se consideran dos niveles de respuesta cualesquiera $k$ y $l$, entonces:\n\n$$\n\\begin{aligned}\nlogit [P(Y \\le k | X = x)] - logit [P(Y \\le l | X = x)] = \\tau_{k} - \\tau_{l} \\\\\n\\frac{odds(P(Y \\le k | X = x))}{odds(P(Y \\le l | X = x))}  =  \\exp(\\tau_{k} - \\tau_{l}) \\implies \\\\\nodds(P(Y \\le k | X = x))  \\propto odds(P(Y \\le l | X = x))\n\\end{aligned}\n$$ {#eq-ordinal2}\n\nEs decir, que la proporcionalidad de $odds$ de dos niveles de respuesta es independiente de los valores concretos de los predictores, por lo que la constante de proporcionalidad debe ser similar para todos ellos.\n\n \n## Modelos Multinivel Generalizados {#sec-multinivel}\n\nUn Modelo Multinivel Generalizado (*\\gls{GLMM}*, *Generalized Linear Mixed Model*), anidado, jerárquico o mixto es un modelo en el que los datos están anidados en una estructura jerárquica. Se utilizan cuando se incumple la hipótesis de independencia entre las observaciones. Por ejemplo, si se quisiera evaluar el rendimiento de varios métodos de enseñanza, se podrían seleccionar aleatoriamente varios colegios participantes y en cada uno de ellos elegir varias clases en las que se impartiría uno de los métodos de enseñanza.  En este caso, los alumnos de una clase no son independientes de los alumnos de otra clase del mismo colegio y también es esperable que los alumnos de un mismo colegio sean más parecidos entre sí que los de otro colegio. Otra situación en la que se viola la condición de independencia entre observaciones es cuando se toman varias medidas del mismo sujeto. Este tipo de experimentos se llaman de medidas repetidas o longitudinales\n^[Hay una diferencia conceptual entre medidas repetidas y longitudinales. Una variable se dice que es longitudinal cuando se toman varias medidas de los sujetos objeto del estudio en diferentes momentos del tiempo. Para que sea considerada de medidas repetidas, las medidas de cada sujeto se toman con distintos niveles de factor. En la práctica la distinción es poco relevante ya que ambas situaciones se parametrizan de la misma forma.]. Cuando se da este supuesto, se considera que las medidas están anidadas en el sujeto [ver @Liu2202]. En un modelo multinivel no es necesario que todas las variables tengan una estructura jerárquica. Se distinguen entonces dos tipos de variables: Las conocidas como de efectos fijos son aquellas que se considera que tienen el mismo efecto en toda la población y, por lo tanto, se debe estimar un único coeficiente. Las variables de efectos aleatorios tienen un coeficiente diferente para cada elemento de la población y se supone que son una muestra de una población mucho mayor, como el caso de seleccionar aleatoriamente una muestra de colegios. Normalmente el coeficiente particular de cada elemento no es de interés para el investigador y se asume que tienen una media centrada en cero. El mayor interés de los efectos aleatorios es la estimación de su matriz de varianzas-covarianzas.\n\nLa ecuación general de un modelo multinivel con dos niveles y un solo predictor con efectos aleatorios es [ver @chen2021, pp. 40]:\n\n$$\n\\begin{aligned}\nNivel\\ 1: & y_{ij}     & = & \\beta_{0j} + \\beta_{1j}x_{1ij} + \\epsilon_{ij} \\\\\nNivel\\ 2: & \\beta_{0j} & = & \\beta_{0} + U_{0j} & (intercepto\\ aleatorio) \\\\\n          & \\beta_{1j} & = & \\beta_{1} + U_{1j} & (pendiente\\ aleatoria) \\\\\n\\end{aligned}\n$$\n\nLos errores del modelo se distribuyen:\n\n$$\n\\begin{aligned}\n\\text{Error intra grupo: } &  \\epsilon_{ij} \\sim N(0, \\sigma^2) \\\\\n\\text{Error entre grupos: } &\n\\begin{pmatrix}\n     U_{0j} \\\\\n     U_{1j} \\\\\n\\end{pmatrix} \n\\sim\nN\n\\begin{pmatrix}\n\\begin{pmatrix}\n     0 \\\\\n     0 \\\\\n\\end{pmatrix},\n\\begin{pmatrix}\n     \\tau_0^2 & \\tau_0\\tau_1\\rho_{01} \\\\\n     \\tau_0\\tau_1\\rho_{01} &  \\tau_1^2 \\\\\n\\end{pmatrix}\n\\end{pmatrix} \n\\end{aligned}\n$$\n\n\ndonde $j$ son los grupos que varían $j = 1,...,J$ ($J$ es el número de grupos); $ij$ es la observación $i$-ésima del grupo $j$ ($i = 1,...,n_j$, $n_j$ es el número de observaciones del grupo $j$). El modelo se compone de una parte fija $\\beta_0 + \\beta_1 x_{1ij}$ y una aleatoria $U_{0j} + U_{1j} x_{1ij} + \\epsilon{ij}$. Los parámetros de este modelo son el intercepto y la pendiente de efectos fijos ($\\beta_0$ y $\\beta_1$), la varianza intra-grupos ($\\sigma^2$), la varianza inter-grupos del intercepto aleatoria ($\\tau_0$) y de la pendiente aleatoria ($\\tau_1$), y la correlación entre intercepto y pendiente aleatorias ($\\rho_{01}$).\n\nEn @gelman2013 [p. 115] se evalúan tres posibilidades a la hora de definir un modelo:\n\n* $Complete\\ pooling$: Consiste en estimar un único parámetro para cada predictor. Es equivalente a un modelo con efectos fijos.\n* $No\\ pooling$: Se estiman tantos parámetros como grupos haya de forma independiente.\n* $Partial\\ pooling$: Es el modelo jerárquico. Es una mezcla de ambos, ya que, aunque se estima un parámetro para cada grupo (como en $no\\ pooling$), esta estimación no es independiente, sino que se supone que las observaciones de un mismo grupo proceden de una misma distribución de probabilidad. Esto se traduce en que se produce una contracción (*\\gls{shrinkage}*) en la estimación de los parámetros hacia la media. Al influir la estimación de unas observaciones en otras, la estimación es de menor valor absoluto que la que resultaría en un modelo de $no\\ pooling$. De esta forma se puede ver el $complete\\ pooling$ y el $no\\ pooling$ como dos casos particulares y extremos del $partial\\ pooling$. La contracción de coeficientes en los modelos multinivel actúa como una regularización que puede evitar el sobreajuste.\n\nLos modelos multinivel requieren supuestos adicionales en el nivel segundo y superiores que son similares a los supuestos para los modelos de efectos fijos [ver @chen2021, p. 43]. Para estimar los parámetros en un modelo multinivel se suele utilizar el método de Máxima Verosimilitud Restringida ($RMLE$), que es una variante de la estimación por Máxima Verosimilitud ($MLE$) en la que se hacen ajustes en los grados de libertad del modelo con efectos aleatorios para corregir el sesgo que se produce al usar $MLE$ en estos modelos.\n\nPara evaluar si la estructura anidada es adecuada se utiliza la Correlación Intra-Clase (*\\gls{ICC}*, Intra-Class Correlation). $ICC$ se puede interpretar como la proporción de la varianza explicada por la estructura de agrupamiento de la población. Se diferencia del Coeficiente de Determinación ($R2$) en que éste es la proporción de la varianza explicada por el modelo completo, mientras que $ICC$ es la varianza explicada por los efectos aleatorios [ver @performance]. La $ICC$ se calcula como el ratio de la varianza entre grupos y la varianza total [ver @chen2021, pp. 29-33]:\n\n$$\nICC = \\rho = \\frac{\\tau^2}{\\tau^2+\\sigma^2}\n$$\n\ndonde $\\tau^2$ es la varianza poblacional entre grupos y $\\sigma^2$ es la varianza de la población dentro del grupo.  $ICC$ oscila entre 0 (ausencia de varianza entre grupos) y 1 (no varianza intra-grupos). Cuanto más proxima a 1 sea $ICC$ mayor evidencia hay de la existencia de una estructura anidada. Sin embargo, no hay un consenso sobre el umbral concreto que debe tener $ICC$ para decidir si es preferible o no la estructura anidada [ver @chen2021, p. 33]. $ICC$ se estima a partir de la varianza de los coeficientes y residual aleatorias calculadas por el modelo. En modelos $GLM$ no se calcula la varianza residual y se recurre a métodos de simulación para estimar $ICC$.\n\n## Modelado bayesiano {#sec-bayesiano}\n\nEl paradigma frecuentista parte de la suposición de que los datos son generados a partir de una variable aleatoria $Y$ y para estimar los coeficientes se maximiza la función de verosimilitud $p(y | \\theta)$ que depende del parámetro desconocido $\\theta$. En el análisis bayesiano se considera que $\\theta$ es una variable aleatoria ya que hay incertidumbre respecto a su valor. Esto se traduce en que se debe asignar una distribución de probabilidad $p(\\theta)$, conocida como distribución a priori, que expresa nuestra creencia sobre los valores que puede tomar $\\theta$. En la inferencia bayesiana se usa la distribución de probabilidad a posteriori $p(\\theta | y)$ que es proporcional al producto de la función de verosimilitud y de la distribución de probabilidad a priori [ver @nicenboim2023]:\n\n\n$$\n\\hbox{Posterior} = \\frac{\\hbox{Likelihood} \\times \\hbox{Prior}}{\\hbox{Marginal Likelihood}}\n\\Rightarrow p(\\theta|y) = \\cfrac{ p(y|\\theta) \\times p(\\theta) }{p(y)} \\propto p(y|\\theta) \\times p(\\theta)\n$$\n\nEn la inferencia bayesiana hay dos fuentes de incertidumbre: Por un lado hay que contar con la variabilidad de $Y$, ya que si se toman varias muestras, los valores $y_i$ obtenidos serán diferentes. Además, existe otra incertidumbre que proviene del desconocimiento del valor de $\\theta$. En la estimación frecuentista, debido a que se utilizan estimaciones puntuales de $\\theta$, no se tiene en cuenta esta incertidumbre. La @eq-ypred se corresponde con la\ndistribución predictiva a posteriori que tiene en consideración ambas incertidumbres:\n$$\n\\begin{aligned}\np(y_{pred}\\mid y ) & = \\int_{\\theta} p(y_{pred}, \\theta \\mid y)\\, d\\theta= \\int_{\\theta} \np(y_{pred}\\mid \\theta,y)p(\\theta \\mid y)\\, d\\theta \\\\ \n& = \\int_{\\theta} p(y_{pred}\\mid \\theta) p(\\theta \\mid y)\\, d\\theta\n\\end{aligned}\n$$ {#eq-ypred}\n\ndonde la última igualdad resulta de la independencia condicional de $y_{pred}$ e $y$ dado $\\theta$ ($y_{pred} \\perp\\!\\!\\!\\perp y \\mid \\theta$).\n\nUna crítica habitual a la inferencia bayesiana es que la elección de la distribución de probabilidad a priori es subjetiva. Aunque es cierto que hay un grado de subjetividad en esta elección, en realidad en el modelado frecuentista hay que tomar ciertas decisiones que también lo son, como por ejemplo la elección del nivel de significación o la forma que adopta la función de verosimilitud. En la práctica, si las observaciones son suficientemente informativas y la distribución a priori es poco informativa, la distribución de probabilidad a priori tendrá poca o nula influencia en la distribución a posteriori ya que estará dominada por la función de verosimilitud y los coeficientes estimados serán muy parecidos en ambos paradigmas. Sin embargo, en lo que diferirán es en la interpretación ya que, por ejemplo, en un modelo bayesiano se pueden interpretar los intervalos de confianza como la probabilidad de que el parámetro esté dentro del intervalo. Por eso a estos intervalos se les conoce como intervalos de credibilidad. Esa interpretación en un modelo frecuentista carecería de sentido ya que los parámetros del modelo no se consideran variables aleatorias y, por lo tanto, tendrán probabilidad 1 si el verdadero valor del parámetro cae dentro del intervalo y 0 si no lo hace. Para obtener la distribución de probabilidad a posteriori normalmente se recurre a métodos del simulación *\\gls{MCMC}* (Métodos de Montecarlo basados en Cadenas de Markov).\n^[En ocasiones se puede obtener una forma análitica de la distribución a posteriori si se elige una adecuada combinación de función de verosimilitud y distribución a priori conocidas como distribuciones conjugadas. Aunque esto evita la utilización de métodos de simulación, restringe las formas posibles de las distribuciones. En la actualidad, con el aumento de la capacidad de cálculo de los ordenadores, normalmente no es necesaria la utilización de distribuciones conjugadas.].\n\nPara comparar modelos entre sí se pueden usar varias medidas [ver @barreda2023]. Por ejemplo, la conocida como *log pointwise predictive density* o densidad predictiva puntal ($lpd$) se calcula como:\n$$\n\\widehat{\\mathrm{lpd}} = \\sum_{i=1}^{N} \\mathrm{log} (p(y_{i} | \\theta))\n$$\n\nLa $lpd$ es la densidad conjunta de observar los datos dada la estructura del modelo y las estimaciones de los parámetros $\\theta$. Aunque las probabilidades a priori no se incluyen en su cálculo, sí influyen en la estimación de $\\theta$ y, por lo tanto, tienen un efecto en los valores de $lpd$. Mayores valores de $lpd$ estarían indicando un mejor modelo. El problema con esta métrica es que se utilizan los datos tanto para estimar el modelo como para seleccionar el mejor modelo. Esto va a producir un sobreajuste y tenderá a favorecer los modelos más complejos. Una métrica mejor es la *expected log pointwise predictive density* o densidad predictiva puntual esperada ($elpd$). Se define en términos de valores fuera de la muestra $\\tilde{y}$ en lugar de con los valores de la muestra $y$:\n\n$$\n\\mathrm{elpd} = \\sum_{i=1}^{N} \\mathbb{E}(\\mathrm{log} (p(\\tilde{y}_i | \\theta)))\n$$\n\nEn la práctica no se puede saber el valor de $elpd$ ya que no se conoce el proceso que genera verdaderos valores $\\tilde{y}$. Una forma de estimar $elpd$ que empíricamente se ha demostrado que funciona es penalizar $lpd$ con el número de parámetros $p$ de forma análoga a lo que se hace en $AIC$ (ver @eq-aic):\n\n$$\n\\widehat{\\mathrm{elpd}} = \\widehat{\\mathrm{lpd}} - \\mathrm{p}\n$$\n \nEl problema es que en modelos multinivel conocer el número de parámetros no es sencillo ya que los parámetros asociados a efectos aleatorios no se pueden considerar que sean completamente independientes. El número efectivo de parámetros va a depender de la importancia de la regresión hacia la media que sufra cada parámetro. Además, en lugar de usar una estimación puntual, se puede utilizar toda la distribución de valores de la simulación. La métrica *widely available information criterion* o \"criterio de información ampliamente disponible\" ($WAIC$) es una forma de estimar $lpd$ que usa toda la distribución de probabilidad a posteriori:\n\n$$\n\\widehat{\\mathrm{lpd}} = \\sum_{i=1}^{n} \\mathrm{log} (\\frac{1}{S} \\sum_{s=1}^{S} p(y_{i} | \\theta^s))\n$$\n\ndonde $S$ es el tamaño de la muestra y el sumatorio interior es la media de densidad en un punto $i$. Para penalizar los modelos más complejos, se usa la varianza de la función de densidad logarítmica:\n\n$$\n\\begin{aligned}\n\\widehat{\\mathrm{elpd}}_{WAIC} &= \\widehat{\\mathrm{lpd}} - \\mathrm{p_{WAIC}} \\\\\n\\mathrm{p_{\\mathrm{WAIC}}} &= \\sum_{i=1}^{n} \\mathrm{Var}_{s=1}^{\\,S}(\\mathrm{log} (p(y_{i} | \\theta^s)))\n\\end{aligned}\n$$\n\nUna forma alternativa de evaluar un modelo es mediante validación cruzada ($CV$, Cross Validation). La validación cruzada más popular es *K-fold-CV*. Con esta técnica se divide el conjunto de datos en $K$ partes y se entrena el modelo separando sucesivamente cada una de las partes que se usan para evaluar el modelo. Los valores estimados resultan de promediar los $K$ resultados. El problema es que cuanto menor sea $K$ más inestables son los resultados obtenidos, y cuanto mayor sea $K$ más veces hay que reentrenar el modelo. Un caso extremo de validación cruzada, que produce gran estabilidad en las estimaciones, es dejar un dato fuera cada vez (*Leave One Out*, $LOO$, $K$ = $N$). La dificultad es que requiere estimar el modelo tantas veces como datos se tengan. Para evitar esto, hay formas de aproximar $\\widehat{\\mathrm{elpd}}$ basadas en $LOO$ sin tener que reentrenar el modelo. La fórmula es la siguiente:\n\n$$\n\\begin{aligned}\n\\widehat{\\mathrm{elpd}}_{LOO} \\approx \\sum_{i=1}^{n} \\mathrm{log} (p(y_{i} | \\theta_{y_{-i}}))\n\\end{aligned}\n$$\n\ndonde $\\theta_{y_{-i}}$ es la estimación de $\\theta$ que resulta tras eliminar la observación $y_{i}$ [ver @gelman2013 pp. 175-176].",
    "supporting": [
      "2_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}