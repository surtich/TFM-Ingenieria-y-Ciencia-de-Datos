```{r}
source("_setup.R")
```

# Marco teórico  y estado del arte {#sec-arte}

## Características del diseño del experimento

El objetivo del estudio es responder a la pregunta de investigación:

::: {.callout-note}
## Pregunta de investigación

¿Son los estudiantes de un curso de creación de materiales accesibles capaces de encontrar diferencias en la calidad del subtitulado de un vídeo?
:::

El diseño del experimento es completamente aleatorizado, de respuesta ordinal, cruzado $AB/BA$ y doble ciego. Es decir, que la asignación de los estudiantes a cada grupo fue aleatoria; cada grupo vio los vídeos en orden inverso; los estudiantes no conocían a priori qué vídeo estaban viendo en cada momento y tampoco se disponía de esta información en el momento de realizar el modelado estadístico.

Un diseño **completamente aleatorizado** [@lawson2015, pp. 18] "garantiza la validez del experimento contra sesgos causados por otras variables ocultas. Cuando las unidades experimentales se asignan aleatoriamente a los niveles de factor de tratamiento, se puede realizar una prueba exacta de la hipótesis de que el efecto del tratamiento es cero utilizando una prueba de aleatorización".

En este trabajo se estudiarán las diferencias existentes entre los dos niveles de subtitulado a través de las respuestas de los alumnos a las escalas de Likert de cada uno de los vídeos. Para ello se propondrán modelos estadísticos adecuados al diseño del experimento. La primera cuestión que debemos abordar es que el diseño sea cruzado.
Siguiendo a @senn2022, para que el ensayo sea de tipo cruzado no sería suficiente intercambiar las secuencias sino que debe ser objeto del ensayo el estudio de las diferencias entre los tratamientos individuales que componen las secuencias. Los principales problemas de un diseño cruzado son el abandono, `drop-out`, de alguno de los participantes y la interacción entre el tratamiento y el periodo o `carry-over`. Además, el análisis estadístico es más complicado y particularmente cuando la respuesta es ordinal y hay más de dos tratamientos. En la misma línea, @lui2016 afirma que "el objetivo principal de un diseño cruzado es estudiar la diferencia entre tratamientos individuales (en lugar de la diferencia entre secuencias de tratamiento). Debido a que cada paciente sirve como su propio control, el diseño cruzado es una alternativa útil al diseño de grupos paralelos para aumentar la potencia".

En un diseño cruzado debemos preocuparnos de la existencia de los efectos periodo y secuencia (o `carry-over`). El **efecto periodo** aplicado al experimento del subtitulado, se producirá si las respuestas del segundo periodo están influidas por haber realizado la primera actividad de subtitulado. El **efecto secuencia** se producirá si las respuestas fueran diferentes cuando se realizan en un orden que cuando se realizan en el otro.

La segunda cuestión de relevancia es que las respuestas a los ítems de una escala de Likert son de **tipo ordinal**. Los test estadísticos ANOVA o MANOVA presuponen que la variable de respuesta es cuantitativa y con distribución normal. Tratar las respuestas a una escala de Likert como si fueran cuantitativas no es correcto por las siguientes razones:

* Los niveles de respuesta no son necesariamente equidistantes: la distancia entre un par de opciones de respuesta puede no ser la misma para todos los pares de opciones de respuesta. Por ejemplo, la diferencia entre "Muy en desacuerdo" y "En desacuerdo" y la diferencia entre "De acuerdo" y "Muy de acuerdo" es de un nivel, pero psicológicamente puede ser percibida de forma diferente por cada sujeto.

* La distribución de las respuestas ordinales puede ser no normal. En particular esto sucederá si hay muchas respuestas en los extremos del cuestionario.

* Las varianzas de las variables no observadas que subyacen a las variables ordinales observadas pueden diferir entre grupos, tratamientos, periodos, etc. 

En @kruschke2018 se han analizado los problemas potenciales de tratar datos ordinales como si fueran cuantitativos constatando que se pueden presentar las siguientes situaciones:

* Se pueden encontrar diferencias significativas entre grupos cuando no las hay: error tipo I.
* Se pueden obviar diferencias cuando en realidad sí existen: error tipo II.
* Incluso se pueden invertir los efectos de un tratamiento.
* También puede malinterpretarse la interacción entre factores.

Otro factor que hay que tener en cuenta es que, al tratarse de un diseño cruzado, es de **medidas repetidas** ya que cada sujeto realiza dos veces el test, uno con cada vídeo y que, por lo tanto, las respuestas a cada test de un mismo sujeto no son independientes. Además, tampoco podemos considerar independientes los ítems que componen el test ya que los ítems pretenden medir la misma variable latente: la calidad del subtitulado.

## Técnicas estadísticas utilizadas {#sec-tecnicas}

En los siguientes apartados se proponen distintas técnicas que pretenden responder al objetivo del trabajo teniendo en cuenta las peculiaridades del diseño del experimento comentadas en el apartado anterior.

### Correlación entre preguntas con el alfa de Cronbach {#sec-cronbach}

Normalmente las preguntas de un cuestionario pretenden medir una variable que está oculta o latente. En nuestro caso es la calidad del subtitulado. Las respuestas a estas preguntas relacionadas deben ser consistentes internamente, es decir, las respuestas deben correlacionarse fuerte y positivamente.

Un índice que se utiliza habitualmente para medir la consistencia interna de un cuestionario es el coeficiente `alfa de Cronbach` [ver @schweinberger2020survey]. Se define de esta forma:

\begin{equation}
\alpha = \frac{N}{N-1} \left(1 - \frac{\sum_{i=1}^{N} s_{i}^{2}}{s^{2}} \right)
\end{equation}

Donde:

* $\alpha$ es el coeficiente `alfa de Cronbach`.
* $N$ es el número de items de la escala de Likert.
* $s_{i}^{2}$ es la varianza de la puntuación del item $i$.
* $s^{2}$ es la varianza total de las puntuaciones de todos los items.

Valores cercanos 1 indican una fuerte correlación en las respuestas y se admite que las preguntas del cuestionario están midiendo la misma variable latente.

## Métodos exploratorios

En esta sección se aplicarán técnicas estadísticas que se basan en tablas de contingencia. Una descripción teórica de este tipo de técnicas se pueden encontrar en @agresti_2018. Un tratamiento aplicado y basado en gráficos, que será el enfoque que seguiremos en este trabajo, es realizado en @frienly2015.


### Asociación de variables con la prueba de homogeneidad $\chi^2$ {#sec-chi2}

La prueba de homogeneidad $\chi^2$ [ver @leton2021] está enmarcarda en el esquema $Nominal \leftarrow Nominal$ y contrasta la hipótesis $H_0$ de que no hay diferencias entre grupos frente a $H_1$ de que existen diferencias. Dada una tabla de contingencia (ver @tbl-contingencia). El valor del estadístico se calcula:


$$
\chi^2 = \sum_{i=1}^r\sum_{j=1}^c\frac{(n_{ij}-c{ij})^2}{c{ij}}
$$

que sigue una $\chi^2_{(r-1)(c-1)}$, donde $c_{ij}$ son las frecuencias esperadas bajo $H_0$ y que se calculan:

$$
c{ij}=\frac{r_ic_j}{n}
$$


|       | X = 1    | X = 2    | X =3     |       |
|-------|---------:|---------:|---------:|------:|
| Y = 1 | $n_{11}$ | $n_{12}$ | $n_{13}$ | $r_1$ |
| Y = 2 | $n_{21}$ | $n_{22}$ | $n_{23}$ | $r_2$ |
|       | $c_1$    | $c_2$    | $c_3$    | $n$   | 
: Tabla de contingencia {#tbl-contingencia}


### Comparación con $Odds\ Ratio$ {#sec-or}

Dada una tabla de contingencia $2\ X\ 2$ (ver @tbl-contingencia-3), el $odds ratio$ poblacional ($OR$) se define como el cociente de las probabilidades complementarias (ver @leton2021). Dado que los factores $Treat$, $Period$ y $Seq$ tienen todos 2 niveles, podemos contrastar si hay interacción entre ellos para cada nivel de respuesta. Para ello contrastamos $H_0$ de que $OR=1$ frente a $H_1$ ($OR\neq1$)
^[Ver @leton2021 para el procedimiento del contraste de hipótesis mediante $log\ OR$.].



|       | X = 1    | X = 0    |
|-------|---------:|---------:|
| Y = 1 | $n_{11}$ | $n_{12}$ |
| Y = 0 | $n_{21}$ | $n_{22}$ |

: Tabla de contingencia {#tbl-contingencia-3}

## Modelos lineales generalizados {#sec-glm}

Los modelos lineales generalizados ($GLM$) son modelos en los que la variable respuesta no es normal. Para especificar un $GLM$ son necesarios tres componentes [ver @agresti_2018, pp. 66-67]:

* Un componente aleatorio: será una distribución de probabilidad de la familia exponencial que se asume que sigue la variable respuesta, $Y$.
* Un componente lineal de predictores.

$$
\alpha+\beta_1x_1+...+\beta_px_p
$$

* Una función de enlace $g$ que relaciona $\mu=E(Y)$ con los predictores, de tal forma que:

$$
g(\mu)=\alpha+\beta_1x_1+...+\beta_px_p
$$


En GLM se realiza maximizando la función de verosimilitud ($MLE$). Es decir, que los coeficientes del modelo son aquellos que maximizan la probabilidad de los datos.

### Regresión Logística {#sec-logistica}

La regresión logística [ver @agresti_2018, pp. 68-69] es un caso particular de $GLM$ donde la variable respuesta, $Y$, es Benoulli o Binomial. Es decir, que $Y$ toma valores 0 ó 1. En una función de Bernoulli de parámetro $\pi$, $E[Y] = P(Y=1) = \pi$. Necesitamos una función que "mapee" los valores que puede tomar el componente lineal de rango $(-\infty, +\infty)$ a los valores que puede tomar $\pi$ en el rango $(0, 1)$. Una función que puede hacer esto es la función $logit$:

$$
logit(Y=1) = log \left[\frac{P(Y=1)}{1-P(Y=1)} \right] = \alpha+\beta_1x_1+...+\beta_px_p
$$

por lo que:

$$
P(Y=1) = \frac{exp^{\alpha + \beta x}}{1 + exp^{\alpha + \beta x}}
$$

Para interpretar los coeficientes, podemos reescribir la ecuación [ver @frienly2015, p. 260]:

$$
\frac{P(Y=1 \mid X = x)}{1-P(Y=1 \mid X = x)} = e^{\alpha}e^{\beta x}
$$

Para una unidad de incremento de $x$:

$$
\frac{P(Y=1 \mid X = x + 1)}{1-P(Y=1 \mid X = x + 1)} = e^{\alpha}e^{\beta(x + 1)}
$$

Dividiendo la segunda ecuación entre la primera:

$$
\frac{\frac{P(Y=1 \mid X = x + 1)}{1-P(Y=1 \mid X = x + 1)}}{\frac{P(Y=1 \mid X = x)}{1-P(Y=1 \mid X = x)}} = \frac{e^{\alpha}e^{\beta(x + 1)}}{e^{\alpha}e^{\beta(x)}} = e^\beta
$$

Es decir:

* $\beta$ es el $log\ OR$ asociado a una unidad de incremento de $x$.
* $\alpha$ es el $log\ odds$ cuando $x=0$.

El contraste de hipótesis para los coeficientes $\beta$:

$$
\begin{aligned}
H_0: \beta_j =  0 \\
H_1: \beta_j \ne  0
\end{aligned}
$$

se puede realizar con el test de Wald:

$$
\begin{aligned}
W & = \frac{\hat\beta_j - 0}{se(\hat\beta_j)} \sim N(0,1)
\end{aligned}
$$


o con el test de razón de verosimilitudes (LRT):

$$
\begin{aligned}
\Lambda &= -2 \log \frac{L(\widehat{reduced})}{L(\widehat{full})}\\
&= -2 \log L(\widehat{reduced}) + 2 \log L(\widehat{full}) \sim \chi^2_r\ (\text{donde r es el número de betas } = 0)
\end{aligned}
$$

Para comparar modelos no anidados, se puede usar el Criterio de Información de Akaike (AIC) o el Criterio de Información Bayesiano (BIC), que se definen:

$$
\begin{aligned}
AIC &: -2 \log L + 2p \\
BIC &: -2 \log L + p \log(n)
\end{aligned}
$$

, donde $L$ es el valor de maxima verosimilitud.


### Regresión Ordinal {#sec-ordinal}


```{r}
#| freeze: true
x <- seq(-4, 4, length.out = 500)
y <- dnorm(x, 0, 1.5)

x_cuts <- c(-2.5, -1.1, 1.8)
x_labels <- c(expression(tau[1]), expression(tau[2]), expression(tau[3]))

data <- tibble(x, y)

g <- data %>% ggplot(aes(x = x, y = y)) +
    geom_line() +
    theme(
        axis.line = element_line(color = "black", linewidth = 1),
        panel.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.line.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.text.x = element_text(color = "black", size = 12, hjust = 0.5, vjust = -3)
    ) +
    ylab(NULL)

cumulative <- g + geom_vline(xintercept = x_cuts) +
    geom_label(size = 2, data = data.frame(x = x_cuts - 0.4, y = rep(0.22, 3), label = paste("Y = ", 1:3, sep = "")), aes(x = x, y = y, label = label)) +
    geom_label(size = 2, x = 2.5, y = 0.22, label = "Y = 4") +
    scale_x_continuous(breaks = x_cuts, labels = x_labels) +
    xlab(expression(tilde(Y))) +
    coord_fixed(ratio = 5)


sequencial <- list()
for (i in 1:3) {
    p <- g + geom_vline(xintercept = x_cuts[i]) +
        geom_label(size = 2, x = x_cuts[i] - 1, y = 0.22, label = paste("Y = ", i, sep = "")) +
        geom_label(size = 2, x = x_cuts[i] + 1, y = 0.22, label = paste("Y > ", i, sep = "")) +
        scale_x_continuous(breaks = x_cuts[i], labels = x_labels[i]) +
        xlab(bquote(tilde(Y)[.(i)])) +
        coord_fixed(ratio = 30)
    sequencial[[i]] <- p
}

adjacent <- list()
for (i in 1:3) {
    p <- g + geom_vline(xintercept = x_cuts[i]) +
        geom_label(size = 2, x = x_cuts[i] - 1, y = 0.22, label = paste("Y = ", i, sep = "")) +
        geom_label(size = 2, x = x_cuts[i] + 1, y = 0.22, label = paste("Y = ", i + 1, sep = "")) +
        scale_x_continuous(breaks = x_cuts[i], labels = x_labels[i]) +
        xlab(bquote(tilde(Y)[.(i)])) +
        coord_fixed(ratio = 30)
    adjacent[[i]] <- p
}
```
  
Las respuestas a los ítems de una escala de Likert son ordinales. La Regresión Ordinal es clase de $GLM$ que comparte muchas similitudes con la regresión logística (ver @sec-logistica). Según @burkner2019 hay tres clases de Regresión Ordinal:

* Regresión Ordinal Acumulativa.
* Regresión Ordinal Secuencial.
* Regresión Ordinal Adyacente.

Nos centraremos en la primera ya que es la más habitual y adecuada para nuestro caso [ver @burkner2019, pp. 23-24]
^[Las regresiones ordinales secuencial y adyacente presuponen que para alcanzar un nivel se ha tenido que pasar previamente por los anteriores, que no es nuestro caso.].
El modelo acumulativo, CM, presupone que la variable ordinal observada, $Y$, proviene de la categorización de una variable latente (no observada) continua, $\tilde{Y}$. Hay $K$ umbrales $\tau_k$ que particionan $\tilde{Y}$ en $K + 1$ categorías ordenadas observables (ver @fig-cumulative). Si asumimos que $\tilde{Y}$ tiene una cierta distribución (por ejemplo, normal) con distribución acumulada $F$, se puede calcular la probabilidad de que $Y$ sea la categoría $k$ de esta forma:

$$Pr(Y = k) = F(\tau_k) - F(\tau_{k-1})$$



```{r}
#| label: fig-cumulative
#| fig-cap: Función latente en una regresión ordinal acumulativa.
#| freeze: true
cumulative
```


Por ejemplo en la @fig-cumulative,

$$Pr(Y = 2) = F(\tau_2) - F(\tau_{1})$$

Si suponemos que $\tilde{Y}$ tiene una relación lineal los predictores:

$$\tilde{Y} = \eta + \epsilon = \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p + \epsilon$$,

y que los errores son $N(0,\sigma^2)$. Entonces la función de probabilidad acumulada de los errores tendrá la misma forma que la de $\tilde{Y}$:

$$\mathrm{Pr}(\epsilon \leq z) = F(z)$$

Y podremos calcular la distribución de probabilidad acumulada de $Y$:

$$\mathrm{Pr}(Y \leq k \mid \eta) = \mathrm{Pr}(\tilde{Y} \leq \tau_k \mid \eta) = \mathrm{Pr}(\eta + \epsilon \leq \tau_k) = \mathrm{Pr}(\epsilon \leq \tau_k - \eta) = F(\tau_k - \eta)$$

Por lo que asumiendo la normalidad de los errores:

$$\mathrm{Pr}(Y = k) = \Phi(\tau_k - \eta) - \Phi(\tau_{k - 1} - \eta)$$

Donde hay que estimar los umbrales y los coeficientes de regresión. La función anterior es la conocida como la función de enlace `probit`. Otra función de enlace popular es la función `logit`. Es la que usaremos en este trabajo por ser más fácil su interpretación
^[En la práctica los coeficientes estimados con las funciones de enlace `probit` y `logit` suelen similares.]. Con esta función de enlace la interpretación de los coeficientes es parecida a la de los coeficientes de la regresión logística. Se parte del supuesto de que el $logit$ de la función de probabilidad es lineal:

$$logit [P(Y \le k)] = \tau_{k} - \eta = \tau_{k} - (\beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p)$$

En ese caso, se puede demostrar fácilmente que, por ejemplo:

$$\frac{\frac{\mathrm{Pr}(Y \leq k \mid \eta)}{\mathrm{Pr}(Y > k \mid \eta)}}{\frac{\mathrm{Pr}(Y \leq k+1 \mid \eta)}{\mathrm{Pr}(Y > k+1 \mid \eta)}} = \exp(\tau_{k} - \tau_{k+1})$$

Y que
^[En la @sec-ordinal-2 se demuestra esta fórmula.]:

$$\frac{\frac{\mathrm{Pr}(Y \leq k \mid x_i = 1)}{\mathrm{Pr}(Y > k \mid x_i = 1)}}{\frac{\mathrm{Pr}(Y \leq k \mid x_i=0)}{\mathrm{Pr}(Y > k \mid x_i = 0)}} = \exp(-\beta_{i})$$

o, equivalentemente:

$$\frac{\frac{\mathrm{Pr}(Y > k \mid x_i = x + 1)}{\mathrm{Pr}(Y \leq k \mid x_i = x + 1)}}{\frac{\mathrm{Pr}(Y > k \mid x_i = x)}{\mathrm{Pr}(Y \leq k \mid x_i = x)}} = \exp(\beta_{i})$$

Es decir, que $\exp(\beta_{i})$ es el $OR$ (cambio en $odds$) de que la variable respuesta esté por encima de una determinada categoría versus estar por debajo de ella para una unidad de incremento del predictor $x_i$. Este modelo se denomina proporcional ya que cada predictor se asume que tiene los mismos efectos sobre todas las categorías de la variable de respuesta ordinal [ver @Liu2202]. Un valor del coeficiente $\beta_i$ positivo indica que la relación entre el predictor $x_i$ y la función de $logit$ es positiva y, por lo tanto, se incrementa la posibilidad de un mayor valor de la variable respuesta. Como veremos, esta suposición se puede relajar y permitir que los coeficientes de todos o de algunos de los predictores sean diferentes para cada pareja consecutiva de valores de respuesta. Tendríamos entonces más parámetros a estimar con una interpretación más compleja.





