{{< include _setup.qmd >}}

# Análisis de la varianza más de un factor.

## Factores subtítulos y preguntas.

Vamos a realizar el análisis pero ahora con dos factores. Hay varias posibilidades. Entre ellas vamos a seleccionar como factores el tratamiento (subtitulado) y las preguntas. Es decir, nos planteamos si hay diferencias entre las preguntas en los distintos tratamientos. Lógicamente esperamos que así sea, pero en el gráfico de interacción (@fig-anove-two) constatamos algunas cuestiones interesantes. Esperaríamos encontrar que uno de los tratamientos debería tener puntuaciones elevadas y el otro bajas ya que una actividad corresponde a un test sobre un vídeo con subtítulos correctos y en la otra hay deficiencias en el subtitulado. Esto es lo que se puede apreciar en el gráfico. Sin embargo en algunas preguntas esto no sucede.:

* En las preguntas 4 y 13 ambos tratamientos tienen puntuaciones altas.
* En las preguntas 15, 16 y 17 ambos tratamientos tienen puntuaciones medias y parecidas.
* Las oscilaciones en el tratamiento B son mayores que en el tratamiento A, que excepto en las preguntas 15, 16 y 17, tiene puntuaciones muy planas y similares a la media.
* Las preguntas 5 y 9 del tratamiento B tienen una puntuación inferior a la media del tratamiento.

```{r}
df <- df_clean %>% mutate(Response = as.numeric(Response))

df %>% ggplot(aes(x = Item, y = Response, color = Treat)) +
    stat_summary(fun = mean, geom = "line", aes(group = Treat)) +
    stat_smooth(method = "lm", formula = y ~ 1, aes(group = Treat), linetype = "dashed", linewidth = 1, se = FALSE, show.legend = FALSE) +
    theme_bw() +
    theme(axis.text.x = element_text(size = 8))
```


Podemos observar que las preguntas 14, 15 y 17 y sobre todo la 16 tienen una gran cantidad de respuestas "No sé / No Contesto".

```{r}
#| echo: false
#| label: fig-anove-two-2
#| fig-cap: Contestación a preguntas con valor No sé / No Contesto por tratamiento.

df_0 %>% ggplot() +
    geom_bar(aes(x = Item, fill = Treat)) +
    theme_bw() +
    scale_x_discrete(drop = FALSE) +
    theme(axis.text.x = element_text(size = 8))
```

TODO: Probablemente los dos gráficos anteriores tienen más sentido en el EDA. Revisar.

El modelo que queremos ajustar es el siguiente:

$$
Y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ijk}
$$

Donde, 
 * $\alpha_i$ es el efecto principal del primer factor al nivel $i$.

 * $\beta_j$ es el efecto principal del segundo factor al nivel $j$.

 * $(\alpha \beta)_{ij}$ es el efecto de la interacción entre los factores a los niveles $i$ y $j$ respectivamente.

 * $\mu$ es el valor de respuesta independiente de nivel de los factores.

 * $\epsilon_{ijk}$ son los errores con distribución $N(0, \sigma^2)$.

 * $Y_{ijk}$ es la observación $k$-ésima de los niveles de factor $i$ y $j$.

Al haber eliminado las respuestas "No sé / No Contesto" y no tener el mismo número de sujetos en ambos grupos. La muestra no está completamente balanceada, pero las diferencias no son excesivas por lo que podemos ignorar este hecho:

```{r}
xtabs(~ Treat + Item, data = df)
```

Ajustamos el modelo y realizamos el contraste de hipótesis:


```{r}
#| echo: true
options(contrasts = c("contr.sum", "contr.poly"))
fit.qt <- aov(Response ~ Treat * Item, data = df)
summary(fit.qt)
```

Concluimos que los tres contrastes son altamente significativos, es decir, que hay efecto principal de ambos factores y también hay un efecto en la interacción entre ellos. Todo esto es bastante razonable:

* En el análisis de varianza de un factor ya habíamos comprobado que la calidad del subtitulado influía en las respuestas.
* Ahora constatamos que el valor de respuesta es diferente para cada pregunta. Es decir, que no todas las preguntas son iguales.
* Por último, que hay una interacción entre subtitulado y pregunta. Es decir, que las diferencias de respuesta entre ambos subtitulados no es constante para todas las preguntas. Como ya habíamos visto, hay preguntas en las que apenas hay diferencia entre las dos calidades de subtitulado y otras en que esta grande.

## Efecto periodo.

El efecto periodo en nuestro diseño consiste en la influencia que tiene en las respuestas el orden en el que se presentan los vídeos. Aquí sólo estamos considerando dos factores, así que podemos comprobar este efecto tanto respecto a las preguntas como respecto a los subtítulos.
```{r}
#| echo: true
options(contrasts = c("contr.sum", "contr.poly"))
fit.pt <- aov(Response ~ Period * Treat, data = df)
summary(fit.pt)
```

```{r}
#| echo: true
options(contrasts = c("contr.sum", "contr.poly"))
fit.pq <- aov(Response ~ Period * Item, data = df)
summary(fit.pq)
```

Encontramos significación estadística del efecto periodo pero no así de sus interacciones con preguntas y subtítulos. Es decir, que en el valor de las respuestas influye el orden en el que se presentan los vídeos pero que esta influencia es constante en las preguntas. De todas la significación estadística no es suficiente ya que en una muestra grande siempre vamos a encontrar significación. Es importante tener también en cuenta la magnitud del efecto periodo. La varianza explicada del efecto periodo es pequeña y por lo tanto también su magnitud.

De todas formas estamos haciendo una ANOVA tipo I que realiza una comparación secuencial de tres modelos desde el más sencillo (modelo) nulo, después con el modelo con el primer factor y luego con el modelo con dos factores. Cuando los datos el modelo secuencial produce diferentes resultados dependiendo del orden de los factores en la fórmula. En datos desbalanceados lo recomendado es hacer un ANOVA tipo III que usa una comparación completa de todos los modelos posibles. Con esta modificación vemos que el efecto periodo ya no es significativo al 95% en el segundo modelo.

```{r}
#| echo: true
library(car)
Anova(fit.pt, type = "III", data = df)
Anova(fit.pq, type = "III", data = df)
```

## Modelo con tres factores

De forma similar se realizará la modelización de más de dos factores. Por ejemplo, el modelo con factores subtítulos, periodo y preguntas será produce efectos significativos en todos los factores.

```{r}
#| echo: true
fit.tpq <- lm( Response ~ Treat + Period + Item, data = df,
            contrasts = list(Treat = contr.sum, Period = contr.sum,
                             Item = contr.sum))
Anova(fit.tpq, type = "III")
```

El test de diferencia de medias y errores ajustados, es otra forma de ver los efectos del subtitulado:

```{r}
#| echo: true
library(emmeans)
lsmeans(fit.tpq, pairwise ~ Treat)
```



## Contrastes

En R es fácil realizar contrastes, por ejemplo supongamos que quisiéramos saber la diferencia en el subtitulado en el periodo 1:

```{r}
#| echo: true
df_contrast <- df %>% mutate(trt.comb = interaction(Treat, Period))
levels(df_contrast$trt.comb)
```

```{r}
#| echo: true
library(multcomp)
fit.comb <- aov(Response ~ trt.comb, data = df_contrast)
fit.glht <- glht(fit.comb,
    linfct = mcp(trt.comb = c(1, -1, 0, 0))
)
confint(fit.glht)
```



# Comprobación de los supuestos de modelo

Vemos que al haber añadido todas las preguntas ahora **QQ-plot** de los residuos del modelo $Response \sim Item + Treat$ se acerca más a la normalidad.

```{r}
#| echo: false
#| fig-cap: QQ-plot para comprobar la normalidad de los residuos.
qqPlot(fit.qt, distribution = "norm")
```
### Comprobación de los supuestos del modelo.

La inferencia estadística solo es válida si se cumplen las siguientes premisas:

* Los errores son independientes.
* Los errores están distribuidos normalmente.
* La varianza del error es constante.
* Los errores tienen una media de cero.

La independencia de los errores se consigue aleatorizando el experimento. Habría que hacer comprobaciones estadísticas de la representatividad de la muestra que no se abordan en este trabajo por no ser objeto del mismo. En cualquier caso, en un estudio cruzado los errores no son independientes y trataremos este problema más adelante.

#### Análisis de residuos

No observamos directamente los errores, $\epsilon_{ij}$, sino una estimación suya que denominamos residuos:

$$
r_{ij} = y_{ij} - \widehat{\mu}_{i}.
$$

Para comprobar la normalidad de los residuos es habitual utilizar un gráfico **QQ-plot** que compara los percentiles de los residuos obtenidos de tras ajustar el modelo con los que resultarían de un distribución normal.

```{r}
#| echo: true
#| fig-cap: QQ-plot para comprobar la normalidad de los residuos.
qqPlot(fit.q18, distribution = "norm")
```

Como era de esperar, una variable de respuesta ordinal no va a producir residuos con distribución normal.

Si los residuos no tienen una distribución normal, el resto de test que hagamos carecen de sentido ya parten de la premisa de que los residuos son normales.